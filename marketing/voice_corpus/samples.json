[
  {
    "source": "regex-beats-llm",
    "type": "opening_problem",
    "text": "Here's a document classification task: given a section of the MCP Transport Specification, determine which parts are mandatory requirements, which are security risks, and which are informational filler your agent can skip.",
    "note": "Opens with a concrete, recognizable task. No preamble."
  },
  {
    "source": "regex-beats-llm",
    "type": "comparative_framing",
    "text": "An LLM can do this. You can prompt Claude or GPT-4 to read the text and classify each section. It will take 2-10 seconds, cost $0.003-0.02 per call, and give you slightly different answers every time you run it. Or you can do it with regex in 3.78 milliseconds. Deterministically. Offline. For free.",
    "note": "Side-by-side comparison with specific numbers. No value judgment — the numbers speak."
  },
  {
    "source": "cognitive-primitive",
    "type": "limitation_acknowledgment",
    "text": "Decompose doesn't know whether 4,000 psf is reasonable for this soil type. A domain expert (human or LLM) does. This is the point. Your LLM handles nuance, cross-referencing, intent, and domain reasoning. Decompose handles everything else — the mechanical work of splitting, classifying, scoring, and extracting — so the LLM can focus on what it's actually good at.",
    "note": "Explicitly states what the tool cannot do, then reframes the limitation as a feature of the architecture."
  },
  {
    "source": "simulation-aware",
    "type": "authority_statement",
    "text": "If your AI system processes documents and someone asks 'why did it flag this section as safety-critical?' — what do you answer? 'The model said so' is not an answer. Not in construction. Not in healthcare. Not in any industry where documents have legal weight and wrong answers have professional liability.",
    "note": "Rhetorical question, then a blunt dismissal. Repetition ('Not in X. Not in Y.') for emphasis. Grounds the argument in professional liability, not abstract quality."
  }
]
