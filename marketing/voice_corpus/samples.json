[
  {
    "source": "regex-beats-llm",
    "type": "opening_problem",
    "text": "Here's a document classification task: given a section of the MCP Transport Specification, determine which parts are mandatory requirements, which are security risks, and which are informational filler your agent can skip.",
    "note": "Opens with a concrete, recognizable task. No preamble."
  },
  {
    "source": "regex-beats-llm",
    "type": "comparative_framing",
    "text": "An LLM can do this. You can prompt Claude or GPT-4 to read the text and classify each section. It will take 2-10 seconds, cost $0.003-0.02 per call, and give you slightly different answers every time you run it. Or you can do it with regex in 3.78 milliseconds. Deterministically. Offline. For free.",
    "note": "Side-by-side comparison with specific numbers. No value judgment — the numbers speak."
  },
  {
    "source": "cognitive-primitive",
    "type": "limitation_acknowledgment",
    "text": "Decompose doesn't know whether 4,000 psf is reasonable for this soil type. A domain expert (human or LLM) does. This is the point. Your LLM handles nuance, cross-referencing, intent, and domain reasoning. Decompose handles everything else — the mechanical work of splitting, classifying, scoring, and extracting — so the LLM can focus on what it's actually good at.",
    "note": "Explicitly states what the tool cannot do, then reframes the limitation as a feature of the architecture."
  },
  {
    "source": "simulation-aware",
    "type": "authority_statement",
    "text": "If your AI system processes documents and someone asks 'why did it flag this section as safety-critical?' — what do you answer? 'The model said so' is not an answer. Not in construction. Not in healthcare. Not in any industry where documents have legal weight and wrong answers have professional liability.",
    "note": "Rhetorical question, then a blunt dismissal. Repetition ('Not in X. Not in Y.') for emphasis. Grounds the argument in professional liability, not abstract quality."
  },
  {
    "source": "brand-refresh",
    "type": "pipeline_narrative",
    "text": "Classification is not the destination. Here's what happens next: units with attention >= 2.0 go to the LLM. Units marked PRESERVE_VERBATIM get stored as-is. Everything else gets skipped. Three lines of code. 70% token reduction.",
    "note": "Shows the 'then what?' pattern. Classification leads to action, not just output. Specific numbers. Concrete steps."
  },
  {
    "source": "brand-refresh",
    "type": "teaching_moment",
    "text": "When you run Decompose on a document and see 47 units with attention scores from 0.0 to 8.0, something clicks. Documents have structure. That structure is classifiable. And classification should happen before reasoning. This is the architectural insight most developers are missing — not because it's hard, but because nobody showed them.",
    "note": "The teaching narrative. Not 'here is a tool' but 'here is an understanding.' The last sentence frames it as a gap in education, not intelligence."
  }
]
